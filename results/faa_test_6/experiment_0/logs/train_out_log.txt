--------Stdout Start--------
Config name: faa_dense_config
Experiment num: 0

Train ins shape: (216789, 6, 3)
Train outs shape: (216789, 3)
Train outs shape: (216789, 3)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 6, 3)]       0                                            
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 6, 3)         12          input_1[0][0]                    
__________________________________________________________________________________________________
flatten (Flatten)               (None, 18)           0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dense (Dense)                   (None, 20)           380         flatten[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 20)           80          dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           210         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 10)           40          dense_1[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 10)           110         batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 10)           40          dense_2[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 10)           110         batch_normalization_3[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 10)           40          dense_3[0][0]                    
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 3)            33          batch_normalization_4[0][0]      
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 3)            33          batch_normalization_4[0][0]      
==================================================================================================
Total params: 1,088
Trainable params: 982
Non-trainable params: 106
__________________________________________________________________________________________________
Train on 216789 samples, validate on 54198 samples
Epoch 1/200
216789/216789 - 14s - loss: 895399420.6959 - dense_4_loss: 333440896.0000 - dense_5_loss: 561969344.0000 - val_loss: 896851913.2154 - val_dense_4_loss: 333998528.0000 - val_dense_5_loss: 562748608.0000
Epoch 2/200
216789/216789 - 14s - loss: 893949662.3877 - dense_4_loss: 332789728.0000 - dense_5_loss: 561150528.0000 - val_loss: 894907458.4644 - val_dense_4_loss: 333135808.0000 - val_dense_5_loss: 561666624.0000
Epoch 3/200
216789/216789 - 13s - loss: 891611357.4601 - dense_4_loss: 331763104.0000 - dense_5_loss: 559853504.0000 - val_loss: 892047417.2715 - val_dense_4_loss: 331870976.0000 - val_dense_5_loss: 560071872.0000
Epoch 4/200
216789/216789 - 13s - loss: 888413780.6502 - dense_4_loss: 330351520.0000 - dense_5_loss: 558063616.0000 - val_loss: 888405994.6029 - val_dense_4_loss: 330267744.0000 - val_dense_5_loss: 558034240.0000
Epoch 5/200
216789/216789 - 13s - loss: 884378736.2280 - dense_4_loss: 328573472.0000 - dense_5_loss: 555808256.0000 - val_loss: 883910883.6146 - val_dense_4_loss: 328287712.0000 - val_dense_5_loss: 555520256.0000
Epoch 6/200
216789/216789 - 15s - loss: 879518725.4521 - dense_4_loss: 326434400.0000 - dense_5_loss: 553085248.0000 - val_loss: 878552928.8172 - val_dense_4_loss: 325930528.0000 - val_dense_5_loss: 552519872.0000
Epoch 7/200
216789/216789 - 14s - loss: 873847060.9944 - dense_4_loss: 323930752.0000 - dense_5_loss: 549896000.0000 - val_loss: 872413920.6648 - val_dense_4_loss: 323217952.0000 - val_dense_5_loss: 549094208.0000
Epoch 8/200
216789/216789 - 14s - loss: 867403575.6893 - dense_4_loss: 321108224.0000 - dense_5_loss: 546290816.0000 - val_loss: 865685123.3288 - val_dense_4_loss: 320253632.0000 - val_dense_5_loss: 545330432.0000
Epoch 9/200
216789/216789 - 12s - loss: 860173871.7421 - dense_4_loss: 317937216.0000 - dense_5_loss: 542232896.0000 - val_loss: 857973777.7258 - val_dense_4_loss: 316895232.0000 - val_dense_5_loss: 540979072.0000
Epoch 10/200
216789/216789 - 14s - loss: 852197288.6285 - dense_4_loss: 314441984.0000 - dense_5_loss: 537754048.0000 - val_loss: 850040661.6037 - val_dense_4_loss: 313407424.0000 - val_dense_5_loss: 536533920.0000
Epoch 11/200
216789/216789 - 13s - loss: 843497937.0066 - dense_4_loss: 310638400.0000 - dense_5_loss: 532860928.0000 - val_loss: 839780295.3650 - val_dense_4_loss: 308909152.0000 - val_dense_5_loss: 530773472.0000
Epoch 12/200
216789/216789 - 13s - loss: 834083353.7687 - dense_4_loss: 306527520.0000 - dense_5_loss: 527564000.0000 - val_loss: 831353926.0188 - val_dense_4_loss: 305227648.0000 - val_dense_5_loss: 526030272.0000
Epoch 13/200
216789/216789 - 13s - loss: 824005795.1746 - dense_4_loss: 302132640.0000 - dense_5_loss: 521877600.0000 - val_loss: 819796629.1184 - val_dense_4_loss: 300221280.0000 - val_dense_5_loss: 519480608.0000
Epoch 14/200
216789/216789 - 13s - loss: 813262121.0890 - dense_4_loss: 297442048.0000 - dense_5_loss: 515803200.0000 - val_loss: 808642211.5839 - val_dense_4_loss: 295348448.0000 - val_dense_5_loss: 513200864.0000
Epoch 15/200
216789/216789 - 13s - loss: 801880388.9198 - dense_4_loss: 292507008.0000 - dense_5_loss: 509379328.0000 - val_loss: 797260953.6482 - val_dense_4_loss: 290403840.0000 - val_dense_5_loss: 506765376.0000
Epoch 16/200
216789/216789 - 13s - loss: 789925279.7114 - dense_4_loss: 287317472.0000 - dense_5_loss: 502608064.0000 - val_loss: 784444108.6328 - val_dense_4_loss: 284825440.0000 - val_dense_5_loss: 499528384.0000
Epoch 17/200
216789/216789 - 13s - loss: 777420042.9467 - dense_4_loss: 281905440.0000 - dense_5_loss: 495506368.0000 - val_loss: 770065557.6144 - val_dense_4_loss: 278621248.0000 - val_dense_5_loss: 491355904.0000
Epoch 18/200
216789/216789 - 13s - loss: 764414968.0527 - dense_4_loss: 276291712.0000 - dense_5_loss: 488114528.0000 - val_loss: 758906665.0099 - val_dense_4_loss: 273802816.0000 - val_dense_5_loss: 485016512.0000
Epoch 19/200
216789/216789 - 13s - loss: 750862089.9612 - dense_4_loss: 270461088.0000 - dense_5_loss: 480407168.0000 - val_loss: 743795273.3902 - val_dense_4_loss: 267287568.0000 - val_dense_5_loss: 476422688.0000
Epoch 20/200
216789/216789 - 13s - loss: 736837126.8815 - dense_4_loss: 264434448.0000 - dense_5_loss: 472399360.0000 - val_loss: 730453276.7444 - val_dense_4_loss: 261557712.0000 - val_dense_5_loss: 468812032.0000
Epoch 21/200
216789/216789 - 13s - loss: 722453578.1375 - dense_4_loss: 258275008.0000 - dense_5_loss: 464186752.0000 - val_loss: 712478529.8445 - val_dense_4_loss: 253846768.0000 - val_dense_5_loss: 458550464.0000
Epoch 22/200
216789/216789 - 13s - loss: 707620625.7822 - dense_4_loss: 251926560.0000 - dense_5_loss: 455687808.0000 - val_loss: 699239283.0667 - val_dense_4_loss: 248243120.0000 - val_dense_5_loss: 450916992.0000
Epoch 23/200
216789/216789 - 13s - loss: 692414199.0785 - dense_4_loss: 245414544.0000 - dense_5_loss: 446995488.0000 - val_loss: 686976855.7340 - val_dense_4_loss: 242842560.0000 - val_dense_5_loss: 444056064.0000
Epoch 24/200
216789/216789 - 13s - loss: 676841294.7771 - dense_4_loss: 238754624.0000 - dense_5_loss: 438085856.0000 - val_loss: 667041063.5268 - val_dense_4_loss: 234455664.0000 - val_dense_5_loss: 432509280.0000
Epoch 25/200
216789/216789 - 13s - loss: 660890128.5092 - dense_4_loss: 231947664.0000 - dense_5_loss: 428941408.0000 - val_loss: 650986985.3087 - val_dense_4_loss: 227703264.0000 - val_dense_5_loss: 423210048.0000
Epoch 26/200
216789/216789 - 13s - loss: 644614287.3392 - dense_4_loss: 225019376.0000 - dense_5_loss: 419602016.0000 - val_loss: 638410351.5466 - val_dense_4_loss: 222220112.0000 - val_dense_5_loss: 416118112.0000
Epoch 27/200
216789/216789 - 13s - loss: 628004179.2891 - dense_4_loss: 217966704.0000 - dense_5_loss: 410023296.0000 - val_loss: 615926767.6286 - val_dense_4_loss: 212670592.0000 - val_dense_5_loss: 403187296.0000
Epoch 28/200
216789/216789 - 13s - loss: 611180358.4083 - dense_4_loss: 210859072.0000 - dense_5_loss: 400316768.0000 - val_loss: 599223946.5799 - val_dense_4_loss: 205566400.0000 - val_dense_5_loss: 393590528.0000
Epoch 29/200
216789/216789 - 13s - loss: 594160462.6707 - dense_4_loss: 203694592.0000 - dense_5_loss: 390458400.0000 - val_loss: 585373567.8996 - val_dense_4_loss: 199791664.0000 - val_dense_5_loss: 385516512.0000
Epoch 30/200
216789/216789 - 13s - loss: 576750672.9133 - dense_4_loss: 196385904.0000 - dense_5_loss: 380373280.0000 - val_loss: 569714326.8968 - val_dense_4_loss: 193201168.0000 - val_dense_5_loss: 376449536.0000
Epoch 31/200
216789/216789 - 13s - loss: 559451555.5679 - dense_4_loss: 189181488.0000 - dense_5_loss: 370273536.0000 - val_loss: 549339977.2969 - val_dense_4_loss: 184814544.0000 - val_dense_5_loss: 364464576.0000
Epoch 32/200
216789/216789 - 13s - loss: 541837324.6530 - dense_4_loss: 181866624.0000 - dense_5_loss: 359972736.0000 - val_loss: 527668312.0623 - val_dense_4_loss: 175728544.0000 - val_dense_5_loss: 351881696.0000
Epoch 33/200
216789/216789 - 13s - loss: 524359737.4283 - dense_4_loss: 174650960.0000 - dense_5_loss: 349703936.0000 - val_loss: 510772706.6156 - val_dense_4_loss: 168610144.0000 - val_dense_5_loss: 342106400.0000
Epoch 34/200
